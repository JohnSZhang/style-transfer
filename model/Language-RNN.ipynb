{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#set hyperparameters\n",
    "max_len = 40\n",
    "step = 2\n",
    "num_units = 128\n",
    "learning_rate = 0.001\n",
    "batch_size = 200\n",
    "epoch = 2\n",
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    '''\n",
    "     open and read text file\n",
    "    '''\n",
    "    text = open(file_name, 'r').read()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text):\n",
    "    '''\n",
    "     featurize the text to train and target dataset\n",
    "    '''\n",
    "    unique_chars = list(set(text))\n",
    "    len_unique_chars = len(unique_chars)\n",
    "\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "\n",
    "    for i in range(0, len(text) - max_len, step):\n",
    "        input_chars.append(text[i:i+max_len])\n",
    "        output_char.append(text[i+max_len])\n",
    "\n",
    "    train_data = np.zeros((len(input_chars), max_len, len_unique_chars))\n",
    "    target_data = np.zeros((len(input_chars), len_unique_chars))\n",
    "\n",
    "    for i , each in enumerate(input_chars):\n",
    "        for j, char in enumerate(each):\n",
    "            train_data[i, j, unique_chars.index(char)] = 1\n",
    "        target_data[i, unique_chars.index(output_char[i])] = 1\n",
    "    return train_data, target_data, unique_chars, len_unique_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(x, weight, bias, len_unique_chars):\n",
    "    '''\n",
    "     define rnn cell and prediction\n",
    "    '''\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    x = tf.reshape(x, [-1, len_unique_chars])\n",
    "    x = tf.split(x, max_len, 0)\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "    prediction = tf.matmul(outputs[-1], weight) + bias\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(predicted):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_data, target_data, unique_chars, len_unique_chars):\n",
    "    '''\n",
    "     main run function\n",
    "    '''\n",
    "    x = tf.placeholder(\"float\", [None, max_len, len_unique_chars])\n",
    "    y = tf.placeholder(\"float\", [None, len_unique_chars])\n",
    "    weight = tf.Variable(tf.random_normal([num_units, len_unique_chars]))\n",
    "    bias = tf.Variable(tf.random_normal([len_unique_chars]))\n",
    "\n",
    "    prediction = rnn(x, weight, bias, len_unique_chars)\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)\n",
    "    cost = tf.reduce_mean(softmax)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    num_batches = int(len(train_data)/batch_size)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print \"----------- Epoch {0}/{1} -----------\".format(i+1, epoch)\n",
    "        count = 0\n",
    "        for _ in range(num_batches):\n",
    "            train_batch, target_batch = train_data[count:count+batch_size], target_data[count:count+batch_size]\n",
    "            count += batch_size\n",
    "            sess.run([optimizer] ,feed_dict={x:train_batch, y:target_batch})\n",
    "        #get on of training set as seed\n",
    "        seed = train_batch[:1:]\n",
    "        save_path = saver.save(sess, \"./tmp/rnn_model\" + str(i) + \".ckpt\")\n",
    "\n",
    "        #to print the seed 40 characters\n",
    "        seed_chars = ''\n",
    "        for each in seed[0]:\n",
    "                seed_chars += unique_chars[np.where(each == max(each))[0][0]]\n",
    "        print \"Seed:\", seed_chars\n",
    "\n",
    "        #predict next 1000 characters\n",
    "        for i in range(1000):\n",
    "            if i > 0:\n",
    "                remove_fist_char = seed[:,1:,:]\n",
    "                seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, len_unique_chars]), axis=1)\n",
    "            predicted = sess.run([prediction], feed_dict = {x:seed})\n",
    "            predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "            probabilities = sample(predicted)\n",
    "            predicted_chars = unique_chars[np.argmax(probabilities)]\n",
    "            seed_chars += predicted_chars\n",
    "        print 'Result:', seed_chars\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seed(text, unique_chars):\n",
    "    len_uniques = len(unique_chars)\n",
    "    letters = []\n",
    "    for i in text:\n",
    "        letter = np.zeros((1, len_uniques))\n",
    "        for j in range(len(unique_chars)):\n",
    "            if i == unique_chars[j]:\n",
    "                letter[0, j] = 1.\n",
    "        letters.append(letter)\n",
    "    return np.array([np.vstack(letters)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model():\n",
    "    text = read_data('shakespeare.txt')\n",
    "    train_data, target_data, unique_chars, len_unique_chars = featurize(text)\n",
    "    x = tf.placeholder(\"float\", [None, max_len, len_unique_chars])\n",
    "    y = tf.placeholder(\"float\", [None, len_unique_chars])\n",
    "    weight = tf.Variable(tf.random_normal([num_units, len_unique_chars]))\n",
    "    bias = tf.Variable(tf.random_normal([len_unique_chars]))\n",
    "\n",
    "    prediction = rnn(x, weight, bias, len_unique_chars)\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)\n",
    "    cost = tf.reduce_mean(softmax)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "    saver.restore(sess, \"./tmp/rnn_model1.ckpt\")\n",
    "    \n",
    "    return unique_chars, prediction, sess, x, len_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, unique_chars, prediction, sess, x, len_unique_chars):\n",
    "    \n",
    "    text_head = ''\n",
    "    pad_head = 0\n",
    "    if len(text) > 40:\n",
    "        text_head = text[:-40]        \n",
    "        text = text[-40:]\n",
    "    elif len(text) < 40:\n",
    "        pad_head  = 40 - len(text)\n",
    "        text = ' ' * pad_head + text\n",
    "        \n",
    "    seed = convert_to_seed(text, unique_chars)\n",
    "    #to print the seed 40 characters\n",
    "    seed_chars = ''\n",
    "    for each in seed[0]:\n",
    "            seed_chars += unique_chars[np.where(each == max(each))[0][0]]\n",
    "    print \"Seed:\", seed_chars\n",
    "\n",
    "    \n",
    "    #predict next 1000 characters\n",
    "    for i in range(50):\n",
    "        if i > 0:\n",
    "            remove_fist_char = seed[:,1:,:]\n",
    "            seed = np.append(remove_fist_char, \n",
    "                             np.reshape(probabilities, [1, 1, len_unique_chars]), axis=1)\n",
    "        predicted = sess.run([prediction], feed_dict = {x:seed})\n",
    "        predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "        probabilities = sample(predicted)\n",
    "        predicted_chars = unique_chars[np.argmax(probabilities)]\n",
    "        seed_chars += predicted_chars\n",
    "        if predicted_chars == '.':\n",
    "            return (text_head + seed_chars)[pad_head:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_chars, predictions, sess, x, len_unique_chars = restore_model()\n",
    "# predict('out fane with you ', unique_chars, predictions, sess, x, len_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = read_data('shakespeare.txt')\n",
    "# train_data, target_data, unique_chars, len_unique_chars = featurize(text)\n",
    "# run(train_data, target_data, unique_chars, len_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = read_data('shakespeare.txt')\n",
    "# train_data, target_data, unique_chars, len_unique_chars = featurize(text)\n",
    "# train_batch, target_batch = train_data[1:5], target_data[1:5]\n",
    "# seed = train_batch[:1:]\n",
    "# print seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.zeros((1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_seed(text, unique_chars):\n",
    "#     len_uniques = len(unique_chars)\n",
    "#     letters = []\n",
    "#     for i in text:\n",
    "#         letter = np.zeros((1, len_uniques))\n",
    "#         for j in range(len(unique_chars)):\n",
    "#             if i == unique_chars[j]:\n",
    "#                 letter[0, j] = 1.\n",
    "#         letters.append(letter)\n",
    "#     return np.array([np.vstack(letters)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_seed('hello', unique_chars).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-237c85de8095>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "----------- Epoch 1/2 -----------\n",
      "Seed: ve him some relief if it be but for that\n",
      "Result: ve him some relief if it be but for that the sang i sar ave , is a a mant .\n",
      "the sirn of as bing and .\n",
      "hare , sar'st i be with the ore master .\n",
      "thou dast the forther .\n",
      "if at a and mear of me .\n",
      "the wird , that ray faries bear the carm .\n",
      "and his a drack the his manger .\n",
      "and it thes a mine the eart hat not asting thes inge .\n",
      "the cront the king and the i so fot not .\n",
      "i have dow not laing the coming .\n",
      "ow come , and not the with .\n",
      "what lough have it the carime .\n",
      "lot it will came to with the 'tring the bearing , the could mane seer .\n",
      "ard the is make the i mase .\n",
      "whore sur him some that dout here and wall my lave and and matir the wast and the faring the ricomed in us not me thou hame dear .\n",
      "the 'ttre i but best is a , and the farting and the mart my mare , ho , my hear in and if the reat ?\n",
      "i have a manter there be the gove there a mare , hour a mant fore theerer bean a my .\n",
      "but now with , but the reave have where i ar ave fruther with suen the mast the farting the mord sow are .\n",
      "the comes .\n",
      "that is all the wind .\n",
      "i have the sive her\n",
      "----------- Epoch 2/2 -----------\n",
      "Seed: ve him some relief if it be but for that\n",
      "Result: ve him some relief if it be but for that well king the will not it as it hear the will of chee , and stain .\n",
      "come , i have not the will so not .\n",
      "i have not that weer , and strow the ' was me .\n",
      "my lord in the word me .\n",
      "i have not mangare .\n",
      "and store and his caust trat his a for stand the wind the sind .\n",
      "he will not hear , i sad , he can not stall lord , and not mone meever .\n",
      "the 'tish be of the 'er man stall not have not mind here .\n",
      "the'r ast for will .\n",
      "ay what is the fair dut the must .\n",
      "i do not do strait there all my live , and indow it a the make the his well the since .\n",
      "i well a mist be dear .\n",
      "i have is bood tid a great were has she that wake her i shall be the manger .\n",
      "as i sable to hear .\n",
      "and stranged all be the sider a waster .\n",
      "i will not bet his daught thou say , but should stright a dong are thim sone did prient .\n",
      "the carry in the bring of the seast , and the comore .\n",
      "i am now , the mist and dees and all the will me .\n",
      "that i san a mear .\n",
      "there is not the dount to the 'tis ward the from the came that will stand thee s\n"
     ]
    }
   ],
   "source": [
    "text = read_data('shakespeare.txt')\n",
    "train_data, target_data, unique_chars, len_unique_chars = featurize(text)\n",
    "run(train_data, target_data, unique_chars, len_unique_chars)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
